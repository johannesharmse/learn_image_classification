{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Image Classification\n",
    "\n",
    "### What is Image Classification?\n",
    "\n",
    "Image classification is the process of determining what is shown in an image.\n",
    "\n",
    "[Silicon Valley - Hot Dog Not Hot Dog](https://www.youtube.com/watch?v=ACmydtFDTGs)\n",
    "\n",
    "We can use deep learning to do this for us. When classifying images using deep learning, we use a convolutional neural network (CNN). CNNs are specifically designed to process images. For this session, we will steer clear of the theory behind CNN's and focus on the practical stuff.\n",
    "\n",
    "![](../additional/img/cnn1.png)\n",
    "\n",
    "## How do CNNs learn to classify?\n",
    "\n",
    "First we need to decide what we want to teach our model.\n",
    "\n",
    "Do we want our model to correctly identify:\n",
    "\n",
    "* Cats and dogs?\n",
    "\n",
    "* Different types of cats?\n",
    "\n",
    "* Different types of dogs?\n",
    "\n",
    "* Different types of flowers?\n",
    "\n",
    "* Everything?\n",
    "\n",
    "CNNs work in a similar way as a human brain (inspired by the way the visual cortex works). If we, as humans, are exposed to something new, it takes time for us to learn what it is.\n",
    "\n",
    "### Can you identify this berry?\n",
    "\n",
    "![](../additional/img/Wild_red_baneberry_1.jpg)\n",
    "\n",
    "If our brain hasn't been exposed to to something, classification becomes a guessing game. This applies to deep learning as well.\n",
    "\n",
    "We need to teach our model what different berries look like.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We need to train our model what the difference is between the different classes.\n",
    "\n",
    "After training, when the model is faced with a new image that it hasn't seen before, it needs to decide for itself what is most likely shown in the image.\n",
    "\n",
    "![](../additional/img/cat_dog.png)\n",
    "\n",
    "![](../additional/img/cat_dog_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training / Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained architechture & Transfer Learning\n",
    "\n",
    "**Architechture used**: [MobileNet](https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html) is a a small efficient convolutional neural network, which is designed to accomodate the restricted resources for an on-device or embedded application.\n",
    "\n",
    "The MobileNet is configurable in two ways:\n",
    "\n",
    "- Input image resolution: 128,160,192, or 224px. Unsurprisingly, feeding in a higher resolution image takes more processing time, but results in better classification accuracy.\n",
    "- The relative size of the model as a fraction of the largest MobileNet: 1.0, 0.75, 0.50, or 0.25.\n",
    "\n",
    "We will use 160 and 0.75 for the first run.\n",
    "```\n",
    "mobilenet_v1_075_160\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining script\n",
    "The retrain script is from the [TensorFlow Hub repo](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py), and we have included in the workshop repo.\n",
    "\n",
    "Before running the script, there are a few arguments worth mentioning:\n",
    "\n",
    "- **bottleneck_dir** : path to cache bottleneck layer values as files\n",
    "- **how_many_training_steps** : How many training steps to run before ending\n",
    "- **summaries_dir** : Where to save training summary logs for TensorBoard\n",
    "- **output_graph** : Where to save the trained graph\n",
    "- **output_labels** : path to save the trained graph's labels\n",
    "- **tfhub_module** : *url* of the model architecture to use from TensorFlow Hub  \n",
    "- **image_dir** : path to labeled images for training\n",
    "\n",
    "You can retrive the whole list of arguments using the following command.\n",
    "\n",
    "```bash\n",
    "python src/retrain.py -h\n",
    "```\n",
    "\n",
    "Let's run the training with the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python src/retrain.py \\\n",
    "    --bottleneck_dir tf_files/bottlenecks \\\n",
    "    --image_dir tf_files/data/train \\\n",
    "    --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_075_160/feature_vector/1 \\\n",
    "    --how_many_training_steps 500 \\\n",
    "    --train_batch_size 25 \\\n",
    "    --summaries_dir tf_files/retrain_logs \\\n",
    "    --output_graph tf_files/output_graph.pb \\\n",
    "    --output_labels tf_files/output_labels.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work? \n",
    "The above script downloads the pre-trained model, adds a new final layer, and trains that layer on the cat/dog photos we provided. It contains two main phases:\n",
    "1. Calculates and caches the bottleneck values for each image\n",
    "2. Actual training of the final layer which makes the classification\n",
    "\n",
    "The techinques that make the training possible is **Transfer Learning**.\n",
    "\n",
    "### Transfer Learning\n",
    "**Transfer learning** is a machine learning method where a model developed for a related task is reused as the starting point for a new model. It has the following benefits\n",
    "\n",
    "- Utilize the power of pre-trained model to extract features from images\n",
    "- Faster...\n",
    "- Less data & less resource (Google: 1000x computing power to replace ml expert)\n",
    "\n",
    "The image below summarize the process. (Image Source: [Google Cloud Next '17](https://www.youtube.com/watch?v=EnFyneRScQ8&feature=youtu.be&t=4m17s) by *Yufeng Guo*)\n",
    "\n",
    "![](../additional/img/retrain.png)\n",
    "\n",
    "### Bottlenecks \n",
    "A **bottleneck** is an informal term we often use for the layer just before the final output layer that actually does the classification (TensorFlow Hub calls this an \"image feature vector\"). This penultimate layer has been trained to output a set of values that's good enough for the classifier to use to distinguish between all the classes it's been asked to recognize.\n",
    "\n",
    "Because every image is reused multiple times during training and calculating each bottleneck takes a significant amount of time, it speeds things up to cache these bottleneck values on disk so they don't have to be repeatedly recalculated. The command you ran saves these files to the `bottlenecks/` directory. If you rerun the script, they'll be reused, so you don't have to wait for this part again.\n",
    "\n",
    "### Actual training\n",
    "You'll see a series of step outputs, each one showing training accuracy, validation accuracy, and the cross entropy. \n",
    "- **training accuracy** : percent of the images used in the current training batch were labeled with the correct class. \n",
    "- **validation accuracy** : the precision on a randomly-selected group of images different from the training.\n",
    "    - **Overfitting** : model may overfit to the noise during training, so we use **validation accuracy** to measure the true performance. If the train accuracy is high but the validation accuracy remains low, that means the network is overfitting and remembering noise\n",
    "- **cross entropy** : a loss function which gives a glimpse into how well the learning process is progressing. It should keep going down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "### What is TensorBoard?\n",
    "\n",
    "TensorBoard is a suite of visualization tools.\n",
    "\n",
    "The goal of Tensorboard is to remove some of the complexity and confusion behind deep learning through visualization.\n",
    "\n",
    "TensorBoard can be used to:\n",
    "\n",
    "* visualize quantitive metrics such as model accuracy, across the training process\n",
    "* visualize how parameters change during the training process\n",
    "* visualise your TensorFlow graph\n",
    "\n",
    "### How do I access TensorBoard?\n",
    "\n",
    "TensorBoard runs in the browser.\n",
    "\n",
    "You can launch TensorBoard by running the following command:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir tf_files/retrain_logs`\n",
    "```\n",
    "\n",
    "Check out the last line. It should say something like:\n",
    "\n",
    "```bash \n",
    "TensorBoard 1.8.0 at http://jharmse:6006 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "This means that TensorBoard is available at `http://jharmse:6006`\n",
    "\n",
    "Type in your equivalent of `jharmse:6006` into your browser (Chrome, Firefox, etc.)\n",
    "\n",
    "You should now see TensorBoard with your browser!\n",
    "\n",
    "**Remember: Once you are done exploring Tensorboard, go to the place where you launched TensorBoard and press `CTRL+C` to quit TensorBoard. Otherwise it will keep on running in the background.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Interesting TensorBoard Features\n",
    "\n",
    "### Scalars\n",
    "\n",
    "Here you can visualize any recording you decided to make during model training. Things you might be interested in visualizing are things like:\n",
    "\n",
    "* model accuracy across iterations\n",
    "* the cross entropy (certainty of model predictions) across iterations\n",
    "\n",
    "Recording can be performed for different processes, like training and validation processes. Visualizing these can help you gain a deeper understanding of the model's performance.\n",
    "\n",
    "For example, if a model's training accuracy is very high towards the end of the iterations, but the validation accuracy is low, it means the model started memorizing the training data instead learning the features of the training images.\n",
    "\n",
    "### Graphs\n",
    "\n",
    "Here you can visually inspect your neural network. For the purposes of this workshop we aren't going to go into the details of this.\n",
    "\n",
    "### Histograms\n",
    "\n",
    "On the histogram tab you can visualize model parameters or value distributions across time.\n",
    "\n",
    "This is really cool since you can see how the parameters change across time to best fit the data it is training on.\n",
    "\n",
    "In our example, where we have two classes, we can see how our distributions become bimodal (two peaks) across iterations. These two peaks are assisting in identifying whether an image is a cat or dog. Each peak relates to one of our two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions using Trained Model\n",
    "\n",
    "Now that we have a model that is fairly good at knowing what a cat and dog looks like, let's give it some \"never seen before\" images to classify on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../tf_files/data/test/4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog 0.9990079\n",
      "cat 0.000992192\n"
     ]
    }
   ],
   "source": [
    "%run -i label_image.py --image ../tf_files/data/test/4.jpg \\\n",
    "    --graph ../tf_files/output_graph.pb \\\n",
    "    --labels ../tf_files/output_labels.txt \\\n",
    "    --input_height 160 \\\n",
    "    --input_width 160 \\\n",
    "    --input_layer Placeholder \\\n",
    "    --output_layer final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../tf_files/data/test/5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 0.99758303\n",
      "dog 0.0024169497\n"
     ]
    }
   ],
   "source": [
    "%run -i label_image.py --image ../tf_files/data/test/10.jpg \\\n",
    "    --graph ../tf_files/output_graph.pb \\\n",
    "    --labels ../tf_files/output_labels.txt \\\n",
    "    --input_height 160 \\\n",
    "    --input_width 160 \\\n",
    "    --input_layer Placeholder \\\n",
    "    --output_layer final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../tf_files/data/test/catdog.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 0.88230264\n",
      "dog 0.11769735\n"
     ]
    }
   ],
   "source": [
    "%run -i label_image.py --image ../tf_files/data/test/catdog.jpg \\\n",
    "    --graph ../tf_files/output_graph.pb \\\n",
    "    --labels ../tf_files/output_labels.txt \\\n",
    "    --input_height 160 \\\n",
    "    --input_width 160 \\\n",
    "    --input_layer Placeholder \\\n",
    "    --output_layer final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "There are other hyperparameters that may affect the model performance.\n",
    "\n",
    "Again, you can read about the whole list of hyperparameters :\n",
    "```bash\n",
    "python src/retrain.py -h\n",
    "```\n",
    "\n",
    "Some of them are not really hyperparameters : i.e. `output_labels`, `summaries_dir` ....\n",
    "\n",
    "Some are :\n",
    "- `learning_rate`: magnitude of the updates to the final layer during training (default: 0.01)\n",
    "- `how_many_training_steps`: how many training steps before the training stop\n",
    "- `train_batch_size`: how many images are used for each training step\n",
    "- ......\n",
    "\n",
    "Try adjusting some of these hyperparameters to improve the validation accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Different Image Dataset (Optional)\n",
    "\n",
    "**Responsible Person: Akshi**\n",
    "\n",
    "**Estimated Duration: TBD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Coding Challenge\n",
    "\n",
    "The School of AI is hands-on! We want everyone to try the things we are learning for themselves! We believe this will help you become more comfortable with the world of AI.\n",
    "\n",
    "We want to create coding challenges to motivate everyone to get their hands dirty with the concepts we are learning.\n",
    "\n",
    "#### This Week's Coding Challenge\n",
    "\n",
    "Use the concepts we learnt today to build your own image classifier! **Build a model that learns images of your own choice!**\n",
    "\n",
    "Submit the application on your own Github profile and share the link on the `Vancouver School of AI` Facebook page. **The deadline is Monday, 17 September**. We will decide on the **most interesting model which does a good job as classifying** (you can use TensorBoard to explore how good of a job your model is doing and tune hyperparameters to increase performance).\n",
    "\n",
    "We will announce the winner on 18 September and make your model known to the world!\n",
    "\n",
    "### Next meetup\n",
    "\n",
    "Next time we will go into more depth about how image classification works - the theory behind these models!\n",
    "\n",
    "This will help you understand and improve your model by applying your own understanding! We will attempt to keep the theory as interesting as poosible!\n",
    "\n",
    "See you next time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
